[[MachineLearning]]
```toc
```

# Gauss Markov Theorem
- the OLS estimator has the lowest sampling variance 
- the distribution of $\epsilon_i$ does not matter, it only matters that $E[x] = \bar{x}=0$


# Supervised Learning: Classification

- previous lectures revolve around estimate parameters of probability distributions
	- conditional expectations of continuous variables
- we will now be predicting a categorical variable class given a set of independent variables
	- class for dependent variable (The thing we are predicting)
	- feature for the independent variables we train and supply the model
- binary classification

## Logistic Regression
$$log\left(\frac{p(\vec{x})}{1-p(\vec{x})}\right)=\hat{p}_{0}+\vec{x}\ \cdot \ \vec{\hat{p}}$$
- take notes later


