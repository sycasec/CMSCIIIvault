del Castillo, Kyle Adrian
CMSC 194.1 - Research Internship I
# Annotated Bibliography 
---

Dhar, P. _et al._ (2021) “Augmented reality in medical education: Students’ experiences and learning outcomes,” _Medical Education Online_, 26(1). Available at: https://doi.org/10.1080/10872981.2021.1953953.

Dhar and colleagues review several augmented reality (AR) based educational programs and how they perform in improving medical training/education, and how they affect delivery of complex information. The authors discuss the benefits of AR technology implemented in medical education as well as its limitations as a tool used for learning and teaching medical education.

The authors point out the main limitation of AR technology which is the cost of designing interactive platforms in AR, as well as the severe lack of resources that would be required to cater to the size of the student body. While AR integrated into education is highly suggested to be a step in the right direction, the current costs simply outweight the benefits.

This article will not form the basis of my research, as it is unrelated to my research topic. The article however is helpful in the sense that the academia may expect more capable scholars with the unstopping improvement of technology integrated into education.  

<br>

--- 

Bac, C.W. _et al._ (2017) “Performance evaluation of a harvesting robot for Sweet Pepper,” _Journal of Field Robotics_, 34(6), pp. 1123–1139. Available at: https://doi.org/10.1002/rob.21709.

In this article, Bac and his colleagues develop a robot that autonomously harvests sweet peppers in a commercial green-house, which was equipped with a Fin Ray and Lip Type end effector in separate instances. The robot's performance in correctly detecting as well as success in grasping the sweet peppers was evaluated. The authors found that the Fin Ray end effector was much more successful in grasping and cutting fruits than the Lip Type end effector.

However, the authors admitted that the robot had major issues in detecting the fruits, which was due to controllable conditions like lighting and different crop conditions. It is also worth noting that the robots can barely detect crops (6% and 2% for Fin Ray and Lip Type respectively) and was only able to improve detection rate up to   about 30% when obstructions were removed. 

While this article does not directly relate to my research topic, it is useful in providing a grasp of how computer vision systems are developed and how they are used to solve problems. 

<div style="page-break-after: always; visibilty: hidden">
</div>

<br>

---

Zapotezny-Anderson, P. and Lehnert, C. (2019) “Towards active robotic vision in agriculture: A deep learning approach to visual servoing in occluded and unstructured protected cropping environments,” _IFAC-PapersOnLine_, 52(30), pp. 120–125. Available at: https://doi.org/10.1016/j.ifacol.2019.12.508.

In  this article, Zapotezny-Anderson and Lenhert integrates a Convolutional Neural Network (CNN) into the 3D Move To See (3DMTS) method creating a Deep-3DMTS approach. This approach is used in robotic crop harvesters which improves the crop harvesters ability to detect crops blocked by leaves or any obstructions. The authors found that the Deep-3DMTS approach had a slight improvement of providing a clear view of the crop.

However, the obvious limitation is that the method is only tested in a simulation, with a single object (a leaf) obstructing the view over the crop. If and when tested in a real world scenario, the success rate may change. More than that, the other obvious limitation is that there is only a single object used to obstruct vision over the crop. Training the CNN may take unimaginably longer when it is made to factor in multiple obstructions.

While this article is closely related to my research topic, it heavily focuses on the integration of Deep Learning in existing crop detection methods. This will not be the basis of my research. However, it is helpful to know more about the applications of Machine Learning in robotics and intelligent systems.

<br>

<br>

---

Iranpak, S., Shahbahrami, A. and Shakeri, H. (2021) “Remote Patient Monitoring and classifying using the internet of things platform combined with cloud computing,” _Journal of Big Data_, 8(1). Available at: https://doi.org/10.1186/s40537-021-00507-w.

In this article, Iranpak, Shahahrami, and Shakeri implement an Internet of Things (IoT) platform that collects patient data from wristwatch sensors. This data is then prioritized and queued into the microcontroller which encrypts and sends the data over to a mobile gateaway, ultimately sending it to a cloud server and deliver information to the patient's family. The authors evalutate this by simulating the entire process and comparing it to other methods. The authors found that their proposed method is 10.41% more accurate than the most current accurate method.

The main limitation in this article is that the accuracies of other methods as depicted by the authors relies solely on their simulation of said IoT methods which could easily be falsified or at the very least, contested. More than that, the proposed method's accuracy data relies only on the authors' simulation which could also be easily falsified or manipulated, and may not reflect real world data. 

While this article is unrelated to my research topic, it may serve a guide in following better research practices. More than that, it is interesting to know that there is constant effort in pushing the limits of IoT.